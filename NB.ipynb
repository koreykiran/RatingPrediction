{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # using numpy package for mathematical operations\n",
    "from numpy import *\n",
    "from numpy.linalg import inv  # used for inversing the matrix\n",
    "import numpy.ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import math\n",
    "import csv\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from operator import add\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist as nF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "data_set = []\n",
    "decision_attributes = []\n",
    "tokens_re = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other required global variables\n",
    "rating_list = []\n",
    "vocab_len = 0\n",
    "n_one = 0\n",
    "n_two = 0\n",
    "n_three = 0\n",
    "n_four = 0\n",
    "n_five = 0\n",
    "n_six = 0\n",
    "n_seven = 0\n",
    "n_eight = 0\n",
    "n_nine = 0\n",
    "n_ten = 0\n",
    "n = 0\n",
    "\n",
    "words_in_one = {}\n",
    "words_in_two = {}\n",
    "words_in_three = {}\n",
    "words_in_four = {}\n",
    "words_in_five = {}\n",
    "words_in_six = {}\n",
    "words_in_seven = {}\n",
    "words_in_eight = {}\n",
    "words_in_nine = {}\n",
    "words_in_ten = {}\n",
    "\n",
    "\n",
    "class_probability = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"Naive-Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    print(\"Loading the dataset...\")\n",
    "    file_read = open(file_path, \"r\")\n",
    "    reader = csv.reader(file_read, dialect='excel')\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        temp = (row[6], row[9], row[12])\n",
    "        \n",
    "        data_set.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n"
     ]
    }
   ],
   "source": [
    "load_file(\"Hotel_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features_reviews = pd.DataFrame(data_set, columns=[\"Negative_Review\", \"Positive_Review\", \"Reviewer_Score\"])\n",
    "target_review = train_data_features_reviews['Reviewer_Score']\n",
    "\n",
    "train_data_features_reviews['Review'] = train_data_features_reviews['Negative_Review'] + \" \" + train_data_features_reviews['Positive_Review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features_reviews.Reviewer_Score = pd.to_numeric(train_data_features_reviews.Reviewer_Score, errors='ignore')\n",
    "train_data_features_reviews.Reviewer_Score = pd.to_numeric(train_data_features_reviews.Reviewer_Score, errors='coerce')\n",
    "\n",
    "train_data_features_reviews.Reviewer_Score = train_data_features_reviews.Reviewer_Score.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoticons string if any for removal\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "# Regex for non alphanumeric characters\n",
    "regex_str = [\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r\"(?:[a-z][a-z\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset to remove to remove stop words by intializing the system\n",
    "def initializeSystem():\n",
    "    print(\"Preparing stop words.\")\n",
    "    stop = set(stopwords.words('english') + punctuation + ['rt', 'via', 'i\\'m', 'us', 'it'])\n",
    "    for x in stop:\n",
    "        stop_words.append(stemmer.stem(lemmatiser.lemmatize(x, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stop words.\n"
     ]
    }
   ],
   "source": [
    "# initializing NLTK functions for preprocessing\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "punctuation = list(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "initializeSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess the Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocess the Data\")\n",
    "\n",
    "# Tokenize will separate each word as tokens\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "# This function removes non-alhanumeric characters, removes the emoticons if present and stems and lemmatize each word\n",
    "def preprocessing(s, lowercase=True):\n",
    "    s = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', s)\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else stemmer.stem(lemmatiser.lemmatize(token.lower(), pos=\"v\")) for\n",
    "                  token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Each row is broken down into space separated words or terms of list and sent for stemming and lemming. Here stop words are removed\n",
    "def process_string(string):\n",
    "    term_stop = [term for term in preprocessing(string) if term not in stop_words and len(str(term)) > 1]\n",
    "    return term_stop\n",
    "\n",
    "# This function is used to send each rows to preprocessing and then combines the result back to a single dataset\n",
    "def preprocess_reviews(reviews):\n",
    "    preprocessed_reviews = []\n",
    "    for index, row in reviews.iterrows():\n",
    "        temp = process_string(str(row[\"Review\"]).lower())\n",
    "        str1 = ' '.join(temp)\n",
    "        temp = [str1]\n",
    "        temp.append(row[\"Reviewer_Score\"])\n",
    "        preprocessed_reviews.append(temp)\n",
    "    return preprocessed_reviews\n",
    "\n",
    "train_reviews = preprocess_reviews(train_data_features_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yx_lines = sc.parallelize(train_reviews)\n",
    "train, test = yx_lines.randomSplit([0.6, 0.4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the Naive Bayes Model\n"
     ]
    }
   ],
   "source": [
    "print(\"Building the Naive Bayes Model\")\n",
    "words_in_train_data1 = train.map(lambda x: tuple(x)).map(lambda x: x[0].split(' '))\n",
    "words_in_train_data2 = [y for x in words_in_train_data1.collect() for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_train_data = Counter(words_in_train_data2)\n",
    "vocab_len = len(words_in_train_data)\n",
    "n = train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg real complaint hotel great great locat surround room amen servic two recommend howev firstli staff upon check confus regard deposit payment staff offer upon checkout refund origin payment make new one bite confus secondli site restaur bite lack well think excel qualiti food anyon vegetarian vegan background even wrap toast sandwich option would great asid minor minor thing fantast spot back return amsterdam',\n",
       " 7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_review_list = train.map(lambda x: (x[1], x[0])).groupByKey().map(lambda x: (x[0],tuple(x[1]))).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rating_review_list.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of words in each class\n",
    "\n",
    "words_in_one1 = sc.parallelize(rating_review_list.lookup(1)).flatMap(lambda xs: [(x, 1) for x in xs]).map(lambda x: x[0].split(' '))\n",
    "words_in_one2 = [y for x in words_in_one1.collect() for y in x]\n",
    "words_in_one = Counter(words_in_one2)\n",
    "n_one = len(words_in_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_two1 = sc.parallelize(rating_review_list.lookup(2)).flatMap(lambda xs: [(x, 1) for x in xs]).map(lambda x: x[0].split(' '))\n",
    "words_in_two2 = [y for x in words_in_two1.collect() for y in x]\n",
    "words_in_two = Counter(words_in_two2)\n",
    "n_two = len(words_in_two2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_three1 = sc.parallelize(rating_review_list.lookup(3)).flatMap(lambda xs: [(x, 1) for x in xs]).map(\n",
    "lambda x: x[0].split(' '))\n",
    "words_in_three2 = [y for x in words_in_three1.collect() for y in x]\n",
    "words_in_three = Counter(words_in_three2)\n",
    "n_three = len(words_in_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_four1 = sc.parallelize(rating_review_list.lookup(4)).flatMap(lambda xs: [(x, 1) for x in xs]).map(\n",
    "lambda x: x[0].split(' '))\n",
    "words_in_four2 = [y for x in words_in_four1.collect() for y in x]\n",
    "words_in_four = Counter(words_in_four2)\n",
    "n_four = len(words_in_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_five1 = sc.parallelize(rating_review_list.lookup(5)).flatMap(lambda xs: [(x, 1) for x in xs]).map(\n",
    "lambda x: x[0].split(' '))\n",
    "words_in_five2 = [y for x in words_in_five1.collect() for y in x]\n",
    "words_in_five = Counter(words_in_five2)\n",
    "n_five = len(words_in_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_six1 = sc.parallelize(rating_review_list.lookup(6)).flatMap(lambda xs: [(x, 1) for x in xs]).map(\n",
    "lambda x: x[0].split(' '))\n",
    "words_in_six2 = [y for x in words_in_six1.collect() for y in x]\n",
    "words_in_six = Counter(words_in_six2)\n",
    "n_six = len(words_in_six)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_seven1 = sc.parallelize(rating_review_list.lookup(7)).flatMap(lambda xs: [(x, 1) for x in xs]).map(\n",
    "lambda x: x[0].split(' '))\n",
    "words_in_seven2 = [y for x in words_in_seven1.collect() for y in x]\n",
    "words_in_seven = Counter(words_in_seven2)\n",
    "n_seven = len(words_in_seven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_eight1 = sc.parallelize(rating_review_list.lookup(8)).flatMap(lambda xs: [(x, 1) for x in xs]).map(\n",
    "lambda x: x[0].split(' '))\n",
    "words_in_eight2 = [y for x in words_in_eight1.collect() for y in x]\n",
    "words_in_eight = Counter(words_in_eight2)\n",
    "n_eight = len(words_in_eight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_nine1 = sc.parallelize(rating_review_list.lookup(9)).flatMap(lambda xs: [(x, 1) for x in xs]).map(\n",
    "lambda x: x[0].split(' '))\n",
    "words_in_nine2 = [y for x in words_in_nine1.collect() for y in x]\n",
    "words_in_nine = Counter(words_in_nine2)\n",
    "n_nine = len(words_in_nine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_ten1 = sc.parallelize(rating_review_list.lookup(10)).flatMap(lambda xs: [(x, 1) for x in xs]).map(\n",
    "lambda x: x[0].split(' '))\n",
    "words_in_ten2 = [y for x in words_in_ten1.collect() for y in x]\n",
    "words_in_ten = Counter(words_in_ten2)\n",
    "n_ten = len(words_in_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_in_nine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(words):\n",
    "    orig_rating = int(words[0])\n",
    "    words = words[1:]\n",
    "\n",
    "    probability1 = 0.0 + math.log10(class_probability.get(1, 1.0))\n",
    "    probability2 = 0.0 + math.log10(class_probability.get(2, 1.0))\n",
    "    probability3 = 0.0 + math.log10(class_probability.get(3, 1.0))\n",
    "    probability4 = 0.0 + math.log10(class_probability.get(4, 1.0))\n",
    "    probability5 = 0.0 + math.log10(class_probability.get(5, 1.0))\n",
    "    probability6 = 0.0 + math.log10(class_probability.get(6, 1.0))\n",
    "    probability7 = 0.0 + math.log10(class_probability.get(7, 1.0))\n",
    "    probability8 = 0.0 + math.log10(class_probability.get(8, 1.0))\n",
    "    probability9 = 0.0 + math.log10(class_probability.get(9, 1.0))\n",
    "    probability10 = 0.0 + math.log10(class_probability.get(10, 1.0))\n",
    "\n",
    "    final_prob = 0\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if (words[i] in words_in_one):\n",
    "            probability1 = probability1 + math.log10(words_in_one[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_two):\n",
    "            probability2 = probability2 + math.log10(words_in_two[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_three):\n",
    "            probability3 = probability3 + math.log10(words_in_three[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_four):\n",
    "            probability4 = probability4 + math.log10(words_in_four[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_five):\n",
    "            probability5 = probability5 + math.log10(words_in_five[words[i]])\n",
    "            \n",
    "        if (words[i] in words_in_six):\n",
    "            probability6 = probability6 + math.log10(words_in_six[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_seven):\n",
    "            probability7 = probability7 + math.log10(words_in_seven[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_eight):\n",
    "            probability8 = probability8 + math.log10(words_in_eight[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_nine):\n",
    "            probability9 = probability9 + math.log10(words_in_nine[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_ten):\n",
    "            probability10 = probability10 + math.log10(words_in_ten[words[i]])\n",
    "\n",
    "    max_prob = probability2\n",
    "    final_prob = 2\n",
    "\n",
    "    if (probability1 == 0.0 and probability2 == 0.0 and probability3 == 0.0 and probability4 == 0.0 and probability5 == 0.0 and probability6 == 0.0 and probability7 == 0.0 and probability8 == 0.0 and probability9 == 0.0 and probability10 == 0.0):\n",
    "        # finalprob=randint(1,5)\n",
    "        final_prob = 0\n",
    "    else:\n",
    "#         if (probability2 > max_prob):\n",
    "#             max_prob = probability2\n",
    "#             final_prob = 2\n",
    "        if (probability3 > max_prob):\n",
    "            max_prob = probability3\n",
    "            final_prob = 3\n",
    "        if (probability4 > max_prob):\n",
    "            max_prob = probability4\n",
    "            final_prob = 4\n",
    "        if (probability5 > max_prob):\n",
    "            max_prob = probability5\n",
    "            final_prob = 5\n",
    "        if (probability6 > max_prob):\n",
    "            max_prob = probability6\n",
    "            final_prob = 6\n",
    "        if (probability7 > max_prob):\n",
    "            max_prob = probability7\n",
    "            final_prob = 7\n",
    "        if (probability8 > max_prob):\n",
    "            max_prob = probability8\n",
    "            final_prob = 8\n",
    "        if (probability9 > max_prob):\n",
    "            max_prob = probability9\n",
    "            final_prob = 9\n",
    "        if (probability10 > max_prob):\n",
    "            max_prob = probability10\n",
    "            final_prob = 10\n",
    "\n",
    "    return (orig_rating, final_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to calculate the \n",
    "def get_accuracy(predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(predictions)):\n",
    "        if predictions[x][0] == predictions[x][1]:\n",
    "            correct += 1\n",
    "    return (correct / float(len(predictions))) * 100.0\n",
    "\n",
    "def get_userreview_prob(words):\n",
    "  \n",
    "    probability2 = 0.0 + math.log10(class_probability.get(2, 1.0))\n",
    "    probability3 = 0.0 + math.log10(class_probability.get(3, 1.0))\n",
    "    probability4 = 0.0 + math.log10(class_probability.get(4, 1.0))\n",
    "    probability5 = 0.0 + math.log10(class_probability.get(5, 1.0))\n",
    "    \n",
    "    probability6 = 0.0 + math.log10(class_probability.get(6, 1.0))\n",
    "    probability7 = 0.0 + math.log10(class_probability.get(7, 1.0))\n",
    "    probability8 = 0.0 + math.log10(class_probability.get(8, 1.0))\n",
    "    probability9 = 0.0 + math.log10(class_probability.get(9, 1.0))\n",
    "    probability10 = 0.0 + math.log10(class_probability.get(10, 1.0))\n",
    "\n",
    "    final_prob = 0\n",
    "\n",
    "    for i in range(len(words)):\n",
    "\n",
    "        if (words[i] in words_in_two):\n",
    "            probability2 = probability2 + math.log10(words_in_two[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_three):\n",
    "            probability3 = probability3 + math.log10(words_in_three[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_four):\n",
    "            probability4 = probability4 + math.log10(words_in_four[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_five):\n",
    "            probability5 = probability5 + math.log10(words_in_five[words[i]])\n",
    "            \n",
    "        if (words[i] in words_in_six):\n",
    "            probability6 = probability6 + math.log10(words_in_six[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_seven):\n",
    "            probability7 = probability7 + math.log10(words_in_seven[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_eight):\n",
    "            probability8 = probability8 + math.log10(words_in_eight[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_nine):\n",
    "            probability9 = probability9 + math.log10(words_in_nine[words[i]])\n",
    "\n",
    "        if (words[i] in words_in_ten):\n",
    "            probability10 = probability10 + math.log10(words_in_ten[words[i]])\n",
    "             \n",
    "    max_prob = probability2\n",
    "    final_prob = 1\n",
    "    \n",
    "    print(max_prob,probability2,probability3,probability4,probability5,probability6,probability7,probability8,probability9,probability10)\n",
    "    \n",
    "    if (probability2 == 0.0 and probability3 == 0.0 and probability4 == 0.0 \n",
    "        and probability5 == 0.0 and probability6 == 0.0 and probability7 == 0.0 and probability8 == 0.0 and\n",
    "       probability9 == 0.0 and probability10 == 0.0):\n",
    "         finalprob=randint(1,5)\n",
    "        #final_prob = 0\n",
    "    else:\n",
    "        if (probability3 > max_prob):\n",
    "            max_prob = probability3\n",
    "            final_prob = 3\n",
    "        if (probability4 > max_prob):\n",
    "            max_prob = probability4\n",
    "            final_prob = 4\n",
    "        if (probability5 > max_prob):\n",
    "            max_prob = probability5\n",
    "            final_prob = 5\n",
    "        if (probability6 > max_prob):\n",
    "            max_prob = probability6\n",
    "            final_prob = 6\n",
    "        if (probability7 > max_prob):\n",
    "            max_prob = probability7\n",
    "            final_prob = 7\n",
    "        if (probability8 > max_prob):\n",
    "            max_prob = probability8\n",
    "            final_prob = 8\n",
    "        if (probability9 > max_prob):\n",
    "            max_prob = probability9\n",
    "            final_prob = 9\n",
    "        if (probability10 > max_prob):\n",
    "            max_prob = probability10\n",
    "            final_prob = 10\n",
    "\n",
    "    return final_prob\n",
    "\n",
    "def pred_rat(s):\n",
    "    test_prediction = sc.parallelize([s])\n",
    "    testwords = test_prediction.map(lambda x: tuple(x)).map(lambda x: get_userreview_prob(x))\n",
    "    pred = testwords.first()\n",
    "    print(\"The predicted rating from NaiveBayes for this review is\",pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the Naive Bayes Model\n",
      "#################\n",
      "Accuracy: with alpha value for NaiveBayes 0.5 is 30.00363645179277%\n"
     ]
    }
   ],
   "source": [
    "alpha=0.5\n",
    "alpha_vocab = alpha * vocab_len\n",
    "\n",
    "words_in_one = dict(sc.parallelize(words_in_one.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_one, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_two = dict(sc.parallelize(words_in_two.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_two, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_three = dict(sc.parallelize(words_in_three.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_three, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_four = dict(sc.parallelize(words_in_four.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_four, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_five = dict(sc.parallelize(words_in_five.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_five, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_six = dict(sc.parallelize(words_in_six.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_six, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_seven = dict(sc.parallelize(words_in_seven.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_seven, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_eight = dict(sc.parallelize(words_in_eight.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_eight, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_nine = dict(sc.parallelize(words_in_nine.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_nine, alpha_vocab)))).collect())\n",
    "\n",
    "words_in_ten = dict(sc.parallelize(words_in_ten.items()).map(lambda x: (x[0], np.divide(np.add(x[1], alpha), np.add(n_ten, alpha_vocab)))).collect())\n",
    "\n",
    "class_probability = dict(rating_review_list.map(lambda x: (x[0], len([item for item in x[-1] if item]))).map(lambda x: (x[0], (x[1] * 1.0) / n)).collect())\n",
    "\n",
    "print(\"Testing the Naive Bayes Model\")\n",
    "\n",
    "test_words1 = test.map(lambda x: (x[1], str(x[1]) + \" \" + x[0])).map(lambda x: (x[0], (x[1:]))).map(lambda x:x[1]) \\\n",
    ".flatMap(lambda xs: [(x, 1) for x in xs]) \\\n",
    ".map(lambda x: x[0].split(' ')) \\\n",
    ".map(lambda x: get_prob(x))\n",
    "\n",
    "predictions = test_words1.collect()\n",
    "\n",
    "\n",
    "accuracy = get_accuracy(predictions)\n",
    "\n",
    "print(\"#################\")\n",
    "print('Accuracy: with alpha value for NaiveBayes '+ repr(alpha)+ ' is ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted rating from NaiveBayes for this review is 9\n"
     ]
    }
   ],
   "source": [
    "pred_rat(\"beautiful view great restaurant awesome food\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
