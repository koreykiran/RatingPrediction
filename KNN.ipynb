{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import string\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from operator import add\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist as nF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark import SparkContext\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset to remove to remove stop words by intializing the system\n",
    "def initializeSystem():\n",
    "    print (\"Preparing stop words.\")\n",
    "    stop = set(stopwords.words('english') + punctuation + ['rt', 'via', 'i\\'m', 'us', 'it'])\n",
    "    for x in stop:\n",
    "        stopWords.append(stemmer.stem(lemmatiser.lemmatize(x, pos=\"v\")))\n",
    "\n",
    "# Tokenize will separate each word as tokens\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "# This function removes non-alhanumeric characters, removes the emoticons if present and stems and lemmatize each word\n",
    "def preprocess(s, lowercase=True):\n",
    "    s = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', s)\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else stemmer.stem(lemmatiser.lemmatize(token.lower(), pos=\"v\")) for\n",
    "                  token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Each row is broken down into space separated words or terms of list and sent for stemming and lemming. Here stop words are removed\n",
    "def processString(string):\n",
    "    # terms_all = [term for term in preprocess(string)]\n",
    "    terms_stop = [term for term in preprocess(string) if term not in stopWords and len(str(term)) > 1]\n",
    "    return ' '.join(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]), 2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1]\n",
    "#         print('res',response)\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "def predic_rating(s):\n",
    "    neighbors = train_features.map(lambda y: (y,euclideanDistance(y, count_vectorizer.transform([s]).toarray()[0], len(y)-2))).sortBy(lambda y: y[1]).map(lambda x:x[0]).take(3)\n",
    "    result = getResponse(neighbors)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String is used to send each row for preprocessing\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "# Regex for non alphanumeric characters\n",
    "regex_str = [\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r\"(?:[a-z][a-z\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stop words.\n"
     ]
    }
   ],
   "source": [
    "stopWords = []\n",
    "decisionAttributes = []\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "punctuation = list(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "initializeSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('Hotel_Reviews.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df=df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame()\n",
    "df1['review'] = df['Negative_Review']+df['Positive_Review']\n",
    "df1['rating'] = df['Reviewer_Score']\n",
    "df1.rating=df1.rating.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Not clean old and sadNo Positive', 5], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize(df1.values)\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = lines.map(lambda x: x[0])\n",
    "rating = lines.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1=review.map(lambda a: processString(str(a).lower())).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int or None, default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_df = 0.7, max_features=500)\n",
    "counts = count_vectorizer.fit_transform(review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=pd.DataFrame(counts.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "features['label']=rating.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 501)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>access</th>\n",
       "      <th>accommod</th>\n",
       "      <th>across</th>\n",
       "      <th>...</th>\n",
       "      <th>within</th>\n",
       "      <th>without</th>\n",
       "      <th>wonder</th>\n",
       "      <th>work</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  12  15  20  30  abl  absolut  access  accommod  across  ...    within  \\\n",
       "0   0   0   0   0   0    0        0       0         0       0  ...         0   \n",
       "1   0   0   0   0   0    0        0       0         0       0  ...         0   \n",
       "2   0   0   0   0   0    0        0       0         0       0  ...         0   \n",
       "3   0   0   0   0   0    0        0       0         0       0  ...         0   \n",
       "4   0   0   0   0   0    0        0       0         0       0  ...         0   \n",
       "\n",
       "   without  wonder  work  worst  worth  would  year  yet  label  \n",
       "0        0       0     0      0      0      0     0    0      5  \n",
       "1        0       0     0      0      0      0     0    0     10  \n",
       "2        0       0     0      0      0      0     0    0      7  \n",
       "3        0       0     0      0      0      0     0    0     10  \n",
       "4        0       0     0      0      0      0     0    0      9  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lines = sc.parallelize(features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = feature_lines.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 24.415713575335655%\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "correct=0\n",
    "for x in test_features.collect():\n",
    "    neighbors = train_features.map(lambda y: (y,euclideanDistance(y, x, len(y)-1))).sortBy(lambda y: y[1]).map(lambda x:x[0]).take(3)\n",
    "    result = getResponse(neighbors)\n",
    "    results.append(result)\n",
    "#     print('> predicted=' + repr(result) + ', actual=' + repr(x[-1]))\n",
    "    if x[-1] == result:\n",
    "        correct += 1\n",
    "accuracy=(correct/float(test_features.count())) * 100.0\n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "predic_rating('The beds were very good, The staff were very supportive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "predic_rating('smoke small poor late limit heat noise bad late service dark wonder temperatur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "predic_rating('awesome rooms great view complimentarty dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 8, 7, 10, 7, 8, 8, 10, 9, 8, 7, 10, 8, 10, 8, 9, 9, 7, 7, 10, 9, 7, 9, 8, 8, 8, 9, 4, 10, 8, 9, 9, 9, 10, 7, 10, 7, 9, 10, 9, 7, 8, 7, 10, 7, 7, 10, 8, 10, 8, 9, 8, 6, 9, 9, 9, 9, 7, 10, 10, 9, 10, 10, 5, 8, 9, 7, 9, 9, 8, 9, 8, 10, 5, 8, 8, 8, 10, 8, 7, 7, 10, 10, 9, 7, 3, 8, 7, 10, 7, 10, 8, 5, 10, 7, 9, 9, 8, 9, 10, 9, 8, 9, 9, 7, 8, 7, 8, 8, 8, 10, 10, 7, 10, 8, 8, 10, 6, 10, 8, 10, 9, 9, 9, 9, 8, 10, 10, 10, 5, 8, 7, 9, 8, 8, 10, 10, 9, 10, 9, 9, 9, 8, 8, 10, 7, 9, 10, 8, 8, 10, 8, 10, 10, 8, 8, 9, 10, 10, 7, 8, 10, 9, 7, 10, 8, 8, 9, 9, 8, 9, 8, 8, 10, 7, 9, 8, 10, 9, 10, 8, 9, 7, 9, 8, 7, 8, 10, 10, 10, 7, 9, 8, 9, 9, 5, 8, 8, 9, 8, 10, 10, 8, 9, 8, 8, 8, 7, 7, 9, 9, 7, 10, 10, 7, 8, 8, 4, 8, 7, 7, 8, 10, 10, 9, 8, 8, 8, 10, 8, 10, 9, 8, 8, 10, 9, 9, 9, 10, 10, 8, 10, 9, 8, 9, 10, 8, 9, 9, 9, 8, 8, 9, 9, 8, 7, 10, 8, 9, 8, 8, 7, 8, 10, 10, 10, 9, 9, 8, 7, 10, 8, 10, 10, 8, 10, 7, 9, 9, 9, 9, 10, 8, 9, 10, 8, 10, 8, 2, 9, 7, 9, 3, 4, 8, 9, 8, 5, 8, 2, 10, 10, 9, 7, 8, 8, 10, 10, 8, 8, 10, 6, 5, 9, 9, 9, 8, 9, 7, 7, 4, 10, 7, 8, 8, 10, 9, 8, 8, 9, 8, 10, 7, 7, 8, 9, 7, 9, 9, 9, 7, 8, 8, 9, 8, 9, 8, 10, 8, 10, 8, 5, 8, 9, 6, 9, 9, 7, 8, 7, 9, 8, 8, 8, 8, 5, 10, 8, 8, 9, 7, 10, 9, 9, 9, 8, 10, 10, 5, 4, 9, 8, 8, 9, 8, 9, 8, 8, 10, 7, 10, 8, 4, 8, 8, 10, 8, 8, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "print(results[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10', u'12', u'15', u'20', u'30', u'abl', u'absolut', u'access', u'accommod', u'across', u'actual', u'air', u'airport', u'alreadi', u'also', u'although', u'alway', u'amaz', u'amen', u'amsterdam', u'anoth', u'anyth', u'apart', u'area', u'around', u'arriv', u'ask', u'atmospher', u'attent', u'attract', u'avail', u'away', u'back', u'bad', u'bag', u'balconi', u'bar', u'barcelona', u'basic', u'bath', u'bathroom', u'beauti', u'bed', u'bedroom', u'best', u'better', u'big', u'bigger', u'birthday', u'bite', u'block', u'book', u'bottl', u'break', u'breakfast', u'brilliant', u'bring', u'bu', u'buffet', u'build', u'busi', u'cafe', u'call', u'car', u'card', u'care', u'carpet', u'center', u'centr', u'central', u'chang', u'channel', u'charg', u'check', u'choic', u'choos', u'citi', u'clean', u'cleanli', u'close', u'cloth', u'coffe', u'cold', u'com', u'come', u'comfi', u'comfort', u'complain', u'complaint', u'complimentari', u'con', u'concierg', u'condit', u'connect', u'consid', u'control', u'conveni', u'cook', u'cool', u'corner', u'corridor', u'cosi', u'cost', u'could', u'coupl', u'courteou', u'cup', u'curtain', u'custom', u'dark', u'date', u'day', u'deal', u'decent', u'decor', u'definit', u'delici', u'design', u'desk', u'despit', u'detail', u'differ', u'difficult', u'din', u'dinner', u'dirti', u'disappoint', u'distanc', u'door', u'doubl', u'drink', u'due', u'earli', u'easi', u'eat', u'effici', u'egg', u'elev', u'els', u'end', u'english', u'enjoy', u'enough', u'entranc', u'equip', u'especi', u'etc', u'euro', u'even', u'ever', u'everi', u'everyth', u'excel', u'except', u'execut', u'expect', u'expens', u'experi', u'extra', u'extrem', u'fabul', u'face', u'facil', u'fact', u'famili', u'fantast', u'far', u'fault', u'feel', u'felt', u'fi', u'find', u'fine', u'first', u'fit', u'fix', u'floor', u'food', u'four', u'free', u'fresh', u'fridg', u'friend', u'friendli', u'front', u'full', u'furnitur', u'garden', u'gener', u'get', u'give', u'glass', u'go', u'good', u'great', u'guest', u'guy', u'gym', u'hair', u'half', u'hall', u'happi', u'hard', u'head', u'hear', u'heat', u'help', u'high', u'highli', u'home', u'hot', u'hotel', u'hour', u'hous', u'housekeep', u'howev', u'huge', u'ideal', u'impress', u'includ', u'incred', u'inform', u'insid', u'interest', u'interior', u'internet', u'iron', u'issu', u'keep', u'kettl', u'key', u'kind', u'king', u'know', u'lack', u'ladi', u'larg', u'last', u'late', u'later', u'leav', u'less', u'let', u'level', u'lift', u'light', u'like', u'limit', u'line', u'littl', u'lobbi', u'local', u'locat', u'london', u'long', u'look', u'lot', u'loud', u'loung', u'love', u'luggag', u'luxuri', u'machin', u'main', u'make', u'manag', u'mani', u'mattress', u'mayb', u'meal', u'mean', u'meet', u'member', u'menu', u'metro', u'milk', u'min', u'mini', u'minut', u'miss', u'modern', u'money', u'morn', u'move', u'much', u'museum', u'near', u'nearbi', u'need', u'neg', u'never', u'new', u'next', u'nice', u'night', u'nois', u'noisi', u'none', u'noth', u'number', u'offer', u'ok', u'old', u'one', u'open', u'option', u'order', u'outsid', u'overal', u'overlook', u'pari', u'park', u'part', u'pay', u'peopl', u'per', u'perfect', u'person', u'phone', u'pillow', u'place', u'pleasant', u'plenti', u'plu', u'plug', u'point', u'polit', u'pool', u'poor', u'posit', u'possibl', u'prefer', u'pretti', u'price', u'problem', u'profession', u'properli', u'provid', u'public', u'put', u'qualiti', u'quiet', u'quit', u'rate', u'rather', u'readi', u'realli', u'reason', u'receiv', u'recept', u'receptionist', u'recommend', u'relax', u'renov', u'request', u'requir', u'restaur', u'return', u'right', u'road', u'roof', u'rooftop', u'room', u'round', u'rude', u'run', u'safe', u'say', u'second', u'see', u'seem', u'select', u'separ', u'serv', u'servic', u'set', u'sever', u'shop', u'short', u'show', u'shower', u'side', u'sight', u'sinc', u'singl', u'sink', u'sit', u'situat', u'size', u'sleep', u'slightli', u'slow', u'small', u'smell', u'smile', u'smoke', u'snack', u'soft', u'someth', u'sound', u'spa', u'space', u'spaciou', u'speak', u'special', u'squar', u'st', u'staff', u'stair', u'standard', u'star', u'start', u'station', u'stay', u'still', u'stop', u'street', u'stuff', u'style', u'stylish', u'subway', u'suit', u'super', u'superb', u'sure', u'surpris', u'swim', u'system', u'tabl', u'take', u'taxi', u'tea', u'tell', u'temperatur', u'terrac', u'terribl', u'thank', u'thing', u'think', u'though', u'three', u'time', u'tini', u'tire', u'toilet', u'toiletri', u'top', u'total', u'touch', u'tourist', u'towel', u'tower', u'town', u'train', u'tram', u'transport', u'travel', u'tri', u'trip', u'tube', u'turn', u'tv', u'twice', u'twin', u'two', u'uncomfort', u'underground', u'understand', u'upgrad', u'upon', u'use', u'valu', u'varieti', u'view', u'visit', u'wait', u'wake', u'walk', u'wall', u'want', u'warm', u'water', u'way', u'weekend', u'welcom', u'well', u'whole', u'wi', u'wifi', u'window', u'wine', u'within', u'without', u'wonder', u'work', u'worst', u'worth', u'would', u'year', u'yet']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import operator\n",
    "# predictions=[]\n",
    "# for x in range(len(test_features1)):\n",
    "#     neighbors = getNeighbors(train_features1, test_features1[x], 3)\n",
    "#     result = getResponse(neighbors)\n",
    "#     predictions.append(result)\n",
    "#     print('> predicted=' + repr(result) + ', actual=' + repr(test_features1[x][-1]))\n",
    "# accuracy = getAccuracy(test_features1, predictions)\n",
    "# print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = sc.parallelize([1,2,3,4, 5])\n",
    "# combinations = rdd.cartesian(rdd)\n",
    "# combinations.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
