{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np  # using numpy package for mathematical operations\n",
    "from numpy import *\n",
    "from numpy.linalg import inv  # used for inversing the matrix\n",
    "import numpy.ma\n",
    "\n",
    "from random import randint\n",
    "import math\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist as nF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "data_set = []\n",
    "decision_attributes = []\n",
    "tokens_re = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-35-18.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc = SparkContext(appName=\"Naive-Bayes-scikitlib\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    print(\"Loading the dataset...\")\n",
    "    file_read = open(file_path, \"r\")\n",
    "    reader = csv.reader(file_read, dialect='excel')\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        temp = (row[6], row[9], row[12])\n",
    "        #temp = (row[0], row[1])\n",
    "        data_set.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n"
     ]
    }
   ],
   "source": [
    "#http://s3.amazonaws.com/cloudcomputingprojectfall18khansoti/Hotel_Reviews.csv\n",
    "load_file(\"Hotel_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features_reviews = pd.DataFrame(data_set, columns=[\"Negative_Review\", \"Positive_Review\", \"Reviewer_Score\"])\n",
    "target_review = train_data_features_reviews['Reviewer_Score']\n",
    "\n",
    "train_data_features_reviews['Review'] = train_data_features_reviews['Negative_Review'] + \" \" + train_data_features_reviews['Positive_Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features_reviews.Reviewer_Score = pd.to_numeric(train_data_features_reviews.Reviewer_Score, errors= 'ignore')\n",
    "train_data_features_reviews.Reviewer_Score = pd.to_numeric(train_data_features_reviews.Reviewer_Score, errors= 'coerce')\n",
    "\n",
    "train_data_features_reviews.Reviewer_Score = train_data_features_reviews.Reviewer_Score.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoticons string if any for removal\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\t\n",
    "# Regex for non alphanumeric characters\n",
    "regex_str = [\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r\"(?:[a-z][a-z\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset to remove to remove stop words by intializing the system\n",
    "def initializeSystem():\n",
    "    print(\"Preparing stop words.\")\n",
    "    stop = set(stopwords.words('english') + punctuation + ['rt', 'via', 'i\\'m', 'us', 'it'])\n",
    "    for x in stop:\n",
    "        stop_words.append(stemmer.stem(lemmatiser.lemmatize(x, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stop words.\n"
     ]
    }
   ],
   "source": [
    "# initializing NLTK functions for preprocessing\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "punctuation = list(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "initializeSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess the Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocess the Data\")\n",
    "\n",
    "# Tokenize will separate each word as tokens\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "# This function removes non-alhanumeric characters, removes the emoticons if present and stems and lemmatize each word\n",
    "def preprocessing(s, lowercase=True):\n",
    "    s = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', s)\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else stemmer.stem(lemmatiser.lemmatize(token.lower(), pos=\"v\")) for\n",
    "                  token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Each row is broken down into space separated words or terms of list and sent for stemming and lemming. Here stop words are removed\n",
    "def process_string(string):\n",
    "    term_stop = [term for term in preprocessing(string) if term not in stop_words and len(str(term)) > 1]\n",
    "    return term_stop\n",
    "\n",
    "# This function is used to send each rows to preprocessing and then combines the result back to a single dataset\n",
    "def preprocess_reviews(reviews):\n",
    "    preprocessed_reviews = []\n",
    "    for index, row in reviews.iterrows():\n",
    "        temp = process_string(str(row[\"Review\"]).lower())\n",
    "        str1 = ' '.join(temp)\n",
    "        temp = [str1]\n",
    "        temp.append(row[\"Reviewer_Score\"])\n",
    "        preprocessed_reviews.append(temp)\n",
    "    return preprocessed_reviews\n",
    "\n",
    "train_reviews = preprocess_reviews(train_data_features_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yx_lines = sc.parallelize(train_reviews)\n",
    "train, test = yx_lines.randomSplit([0.6, 0.4])\n",
    "\n",
    "target_rating = train.map(lambda x: x[1]).collect()\n",
    "target_review = train.map(lambda x: x[0]).collect()\n",
    "\n",
    "test_review = test.map(lambda x: x[0]).collect()\n",
    "test_rating = test.map(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function is used to send each row for preprocessing\n",
    "def preprocess_reviews_lib(reviews):\n",
    "    preprocessed_reviews_lib = []\n",
    "    for a in reviews:\n",
    "        preprocessed_reviews_lib.append(process_string(str(a).lower()))\n",
    "    return preprocessed_reviews_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send training and test set reviews separately for preprocessing\n",
    "train_reviews = preprocess_reviews_lib(target_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get decision attrubites\n",
    "def get_decision_attributes(processed_reviews):\n",
    "    to_count = []\n",
    "    for a in processed_reviews:\n",
    "        to_count.append(\" \".join(a))\n",
    "    str1 = \"\"\n",
    "    for a in to_count:\n",
    "        str1 += \"\".join(a)\n",
    "    x = Counter(str1.split(\" \"))\n",
    "    for (k, v) in x.most_common(min(500, len(x))):\n",
    "        decision_attributes.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_decision_attributes(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare sparse matrix\n",
    "def prepare_sparse_matrix(processed_reviews):\n",
    "    sparse_matrix = []\n",
    "    for cr in processed_reviews:\n",
    "        new_cr = [0] * len(decision_attributes)\n",
    "        for word in cr:\n",
    "            if word in decision_attributes:\n",
    "                index = decision_attributes.index(word)\n",
    "                new_cr[index] += 1\n",
    "            else:\n",
    "                pass\n",
    "        sparse_matrix.append(new_cr)\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse matrix for the both and test and training dataset\n",
    "train_sparse_matrix = prepare_sparse_matrix(train_reviews)\n",
    "test_sparse_matrix = prepare_sparse_matrix(preprocess_reviews_lib(test_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = pd.DataFrame(train_sparse_matrix, columns=decision_attributes)\n",
    "test_data_features = pd.DataFrame(test_sparse_matrix, columns=decision_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fit the model built by the scikit learn library\n",
    "def fitter(func_value):\n",
    "    nb_func_value = str(func_value)\n",
    "    nb_func_value = nb_func_value.partition(\"(\")[0]\n",
    "    func_value.fit(data_features, target_rating)\n",
    "    s = f = 0\n",
    "    predicted_rating = list(func_value.predict(test_data_features))\n",
    "    for i in range(len(predicted_rating)):\n",
    "        if predicted_rating[i] == test_rating[i]:\n",
    "            s += 1\n",
    "        else:\n",
    "            f += 1\n",
    "    print(\"Prediction rate by using \" + nb_func_value + \" is: \" + str(float(s) / len(predicted_rating) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_rat(s):\n",
    "    user_review_matrix = prepare_sparse_matrix([process_string(str(s).lower())])\n",
    "    user_review_features = pd.DataFrame(user_review_matrix, columns=decision_attributes)\n",
    "    print(clf1.predict(user_review_matrix)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction rate by using MultinomialNB is: 35.5298610169\n"
     ]
    }
   ],
   "source": [
    "# calculate multinomial naive bayes and fit the data to the model\n",
    "clf1 = MultinomialNB()\n",
    "fitter(clf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "pred_rat(\"awesome food great view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "pred_rat(\"bad food bad view\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
