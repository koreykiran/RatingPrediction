{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from operator import add\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist as nF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark import SparkContext\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('Hotel_Reviews.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df=df[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame()\n",
    "df1['review'] = df['Negative_Review']+df['Positive_Review']\n",
    "df1['rating'] = df['Reviewer_Score']\n",
    "df1.rating=df1.rating.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Room I got should be a broom room or some staff room it was the size of a single bed so small and with a view on some giant pipe horrific this room was 170  This is a terrible hotel even the location does not make up for it in front of the hotel there is some magazine after a long discussion I got a transfer to a nice hotel which was normal ',\n",
       "       3], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize(df1.values)\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = lines.randomSplit([0.6, 0.4])\n",
    "train_review = train.map(lambda x: x[0])\n",
    "train_rating = train.map(lambda x: x[1])\n",
    "test_review = test.map(lambda x: x[0])\n",
    "test_rating = test.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset to remove to remove stop words by intializing the system\n",
    "def initializeSystem():\n",
    "    print (\"Preparing stop words.\")\n",
    "    stop = set(stopwords.words('english') + punctuation + ['rt', 'via', 'i\\'m', 'us', 'it'])\n",
    "    for x in stop:\n",
    "        stopWords.append(stemmer.stem(lemmatiser.lemmatize(x, pos=\"v\")))\n",
    "\n",
    "# Tokenize will separate each word as tokens\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "# This function removes non-alhanumeric characters, removes the emoticons if present and stems and lemmatize each word\n",
    "def preprocess(s, lowercase=True):\n",
    "    s = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', s)\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else stemmer.stem(lemmatiser.lemmatize(token.lower(), pos=\"v\")) for\n",
    "                  token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Each row is broken down into space separated words or terms of list and sent for stemming and lemming. Here stop words are removed\n",
    "def processString(string):\n",
    "    # terms_all = [term for term in preprocess(string)]\n",
    "    terms_stop = [term for term in preprocess(string) if term not in stopWords and len(str(term)) > 1]\n",
    "    return ' '.join(terms_stop)\n",
    "\n",
    "def convertReviews(reviews):\n",
    "    convertedReviews = []\n",
    "    for index, row in reviews.iterrows():\n",
    "        temp = processString(str(row[\"review\"]).lower())\n",
    "        str1 = ' '.join(temp)\n",
    "        temp = [str1]\n",
    "        temp.append(row[\"rating\"])\n",
    "        # print temp\n",
    "        convertedReviews.append(temp)\n",
    "        # convertedReviews = convertedReviews + ',' + processString(str(row[\"review\"]).lower()) + ',' + row[\"rating\"]\n",
    "    return convertedReviews\n",
    "\n",
    "def convertReviewsLib(reviews):\n",
    "    convertedReviews = []\n",
    "    for a in reviews:\n",
    "        convertedReviews.append(processString(str(a).lower()))\n",
    "    return convertedReviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String is used to send each row for preprocessing\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "# Regex for non alphanumeric characters\n",
    "regex_str = [\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r\"(?:[a-z][a-z\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stop words.\n"
     ]
    }
   ],
   "source": [
    "stopWords = []\n",
    "decisionAttributes = []\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "punctuation = list(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "initializeSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review1=train_review.map(lambda a: processString(str(a).lower())).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int or None, default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df=10,max_df=0.9,max_features=500\n",
    "count_vectorizer = CountVectorizer(max_df = 0.7, max_features=500)\n",
    "counts = count_vectorizer.fit_transform(train_review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFeatures=pd.DataFrame(counts.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59883, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>access</th>\n",
       "      <th>accommod</th>\n",
       "      <th>across</th>\n",
       "      <th>actual</th>\n",
       "      <th>...</th>\n",
       "      <th>wi</th>\n",
       "      <th>wifi</th>\n",
       "      <th>window</th>\n",
       "      <th>within</th>\n",
       "      <th>without</th>\n",
       "      <th>wonder</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  15  20  30  abl  absolut  access  accommod  across  actual  ...   wi  \\\n",
       "0   0   0   0   0    0        0       0         0       0       0  ...    0   \n",
       "1   0   0   0   0    0        0       0         0       0       0  ...    0   \n",
       "2   0   0   0   0    0        0       0         0       0       0  ...    0   \n",
       "3   0   0   0   0    0        0       0         0       0       0  ...    0   \n",
       "4   0   0   0   0    0        0       0         0       0       0  ...    0   \n",
       "\n",
       "   wifi  window  within  without  wonder  work  worth  would  year  \n",
       "0     0       0       0        0       0     0      0      0     0  \n",
       "1     0       0       0        0       0     1      0      1     0  \n",
       "2     0       0       0        0       0     0      0      0     0  \n",
       "3     0       0       0        0       0     0      0      0     0  \n",
       "4     0       0       0        0       0     0      0      0     0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=20, weights='distance', algorithm='brute')\n",
    "clf.fit(dataFeatures, train_rating.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review1 = test_review.map(lambda a: processString(str(a).lower())).collect()\n",
    "testfeatures = count_vectorizer.transform(test_review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataFeatures = pd.DataFrame(testfeatures.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7ff437c99350>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.broadcast(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = sc.parallelize(testDataFeatures.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = testdf.map(lambda x: clf.predict([x])[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.308048956801356"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_rating.collect(), results)\n",
    "\n",
    "def predic_rating(s):\n",
    "    print(clf.predict(count_vectorizer.transform([s]).toarray())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "predic_rating('The beds were very good, The staff were very supportive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "predic_rating('smoke small poor late limit heat noise bad late service dark wonder temperatur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "predic_rating('awesome rooms great view complimentarty dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 6, 9, 10, 5, 7, 9, 10, 9, 9, 9, 9, 8, 9, 10, 8, 10, 8, 9, 9, 10, 7, 7, 9, 9, 7, 8, 9, 9, 9, 7, 9, 9, 10, 7, 10, 10, 10, 9, 8, 9, 10, 9, 9, 9, 8, 9, 7, 9, 9, 7, 10, 10, 5, 9, 9, 8, 10, 8, 9, 9, 5, 9, 10, 9, 5, 8, 9, 9, 10, 9, 8, 9, 9, 9, 10, 8, 9, 9, 5, 9, 8, 10, 5, 10, 8, 10, 8, 10, 7, 10, 9, 7, 8, 7, 7, 9, 9, 9, 10, 10, 8, 7, 10, 7, 7, 10, 9, 3, 6, 5, 9, 7, 9, 9, 9, 10, 7, 10, 10, 7, 8, 10, 7, 9, 10, 10, 7, 7, 9, 9, 7, 9, 7, 10, 7, 10, 7, 10, 7, 7, 7, 9, 8, 10, 10, 7, 7, 10, 7, 9, 10, 10, 10, 7, 7, 10, 7, 9, 10, 10, 10, 8, 8, 6, 10, 7, 6, 7, 9, 10, 10, 9, 8, 9, 7, 9, 7, 7, 8, 9, 5, 9, 7, 9, 8, 7, 8, 7, 10, 9, 7, 8, 10, 5, 9, 7, 7, 7, 10, 9, 7, 8, 7, 7, 8, 9, 10, 8, 9, 7, 7, 7, 10, 9, 10, 7, 7, 7, 7, 7, 9, 7, 9, 7, 10, 7, 7, 5, 9, 9, 9, 10, 7, 7, 5, 8, 10, 10, 9, 5, 10, 6, 5, 10, 10, 7, 10, 7, 9, 10, 10, 9, 10, 9, 10, 10, 9, 8, 8, 9, 10, 10, 9, 8, 9, 10, 10, 10, 10, 10, 8, 7, 7, 9, 7, 5, 10, 8, 10, 10, 8, 10, 9, 5, 7, 10, 9, 7, 7, 9, 7, 9, 10, 7, 9, 7, 9, 7, 8, 8, 10, 8, 10, 8, 10, 8, 10, 10, 8, 9, 9, 8, 9, 10, 9, 9, 8, 8, 8, 7, 9, 8, 9, 10, 10, 9, 9, 10, 10, 9, 7, 7, 10, 8, 7, 9, 9, 7, 10, 7, 7, 7, 10, 7, 9, 9, 7, 10, 10, 7, 9, 2, 8, 8, 9, 9, 7, 9, 7, 10, 9, 9, 10, 7, 9, 9, 7, 7, 7, 8, 9, 9, 9, 8, 10, 10, 9, 8, 8, 5, 9, 7, 10, 5, 9, 9, 9, 10, 8, 7, 10, 7, 8, 5, 9, 9, 8, 10, 7]\n"
     ]
    }
   ],
   "source": [
    "print(results[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10', u'15', u'20', u'30', u'abl', u'absolut', u'access', u'accommod', u'across', u'actual', u'add', u'addit', u'air', u'airport', u'almost', u'also', u'although', u'alway', u'amaz', u'amen', u'amsterdam', u'anoth', u'anyth', u'apart', u'appreci', u'area', u'around', u'arriv', u'ask', u'atmospher', u'attent', u'attract', u'avail', u'away', u'back', u'bad', u'bag', u'balconi', u'bar', u'barcelona', u'basic', u'bath', u'bathroom', u'beauti', u'bed', u'bedroom', u'best', u'better', u'big', u'bigger', u'birthday', u'bite', u'block', u'book', u'bottl', u'break', u'breakfast', u'brilliant', u'bring', u'bu', u'buffet', u'build', u'busi', u'cafe', u'call', u'car', u'card', u'care', u'carpet', u'center', u'centr', u'central', u'chang', u'charg', u'check', u'choic', u'choos', u'citi', u'clean', u'cleanli', u'close', u'club', u'coffe', u'cold', u'com', u'come', u'comfi', u'comfort', u'compar', u'complain', u'complaint', u'complimentari', u'con', u'concierg', u'condit', u'connect', u'consid', u'control', u'conveni', u'cook', u'cool', u'corner', u'corridor', u'cosi', u'cost', u'could', u'coupl', u'curtain', u'custom', u'dark', u'date', u'day', u'deal', u'decent', u'decor', u'definit', u'delici', u'design', u'desk', u'despit', u'detail', u'didnt', u'differ', u'difficult', u'din', u'dinner', u'direct', u'dirti', u'disappoint', u'distanc', u'door', u'doubl', u'drink', u'due', u'earli', u'easi', u'eat', u'effici', u'egg', u'either', u'elev', u'els', u'end', u'english', u'enjoy', u'enough', u'entranc', u'equip', u'especi', u'etc', u'euro', u'even', u'ever', u'everi', u'everyth', u'excel', u'except', u'execut', u'expect', u'expens', u'experi', u'extra', u'extrem', u'fabul', u'face', u'facil', u'fact', u'famili', u'fantast', u'far', u'fault', u'feel', u'felt', u'fi', u'find', u'fine', u'first', u'fit', u'fix', u'floor', u'food', u'free', u'fresh', u'fridg', u'friend', u'friendli', u'front', u'fruit', u'full', u'furnitur', u'garden', u'gener', u'get', u'give', u'glass', u'go', u'good', u'great', u'guest', u'gym', u'hair', u'half', u'hall', u'happi', u'hard', u'hear', u'heat', u'help', u'high', u'highli', u'home', u'hot', u'hotel', u'hour', u'hous', u'housekeep', u'howev', u'huge', u'hyde', u'ideal', u'impress', u'includ', u'incred', u'inform', u'insid', u'instead', u'interior', u'internet', u'iron', u'issu', u'keep', u'kettl', u'key', u'kind', u'know', u'lack', u'ladi', u'larg', u'last', u'late', u'later', u'leav', u'less', u'let', u'level', u'lift', u'light', u'like', u'limit', u'line', u'littl', u'lobbi', u'local', u'locat', u'london', u'long', u'look', u'lot', u'loud', u'loung', u'love', u'luggag', u'luxuri', u'machin', u'main', u'make', u'manag', u'mani', u'mattress', u'mayb', u'meal', u'mean', u'meet', u'member', u'mention', u'menu', u'metro', u'middl', u'milk', u'min', u'mini', u'minut', u'miss', u'modern', u'money', u'morn', u'move', u'much', u'museum', u'near', u'nearbi', u'need', u'neg', u'never', u'new', u'next', u'nice', u'night', u'nois', u'noisi', u'none', u'noth', u'offer', u'ok', u'old', u'one', u'open', u'option', u'order', u'outsid', u'overal', u'pari', u'park', u'part', u'pay', u'peopl', u'per', u'perfect', u'person', u'phone', u'pillow', u'place', u'pleasant', u'plenti', u'plu', u'point', u'polit', u'pool', u'poor', u'posit', u'possibl', u'pretti', u'price', u'problem', u'profession', u'properli', u'properti', u'provid', u'public', u'put', u'qualiti', u'quiet', u'quit', u'rate', u'rather', u'reach', u'readi', u'real', u'realli', u'reason', u'receiv', u'recept', u'receptionist', u'recommend', u'relax', u'renov', u'request', u'reserv', u'restaur', u'return', u'right', u'road', u'roof', u'rooftop', u'room', u'rude', u'run', u'safe', u'say', u'second', u'see', u'seem', u'select', u'separ', u'serv', u'servic', u'set', u'sever', u'shop', u'short', u'show', u'shower', u'side', u'sinc', u'singl', u'sink', u'sit', u'site', u'situat', u'size', u'sleep', u'slightli', u'slow', u'small', u'smell', u'smile', u'smoke', u'soft', u'someon', u'someth', u'sound', u'spa', u'space', u'spaciou', u'speak', u'special', u'spend', u'squar', u'staff', u'stair', u'standard', u'star', u'start', u'station', u'stay', u'step', u'still', u'stop', u'street', u'stuff', u'style', u'stylish', u'suit', u'super', u'superb', u'sure', u'surpris', u'swim', u'system', u'tabl', u'take', u'tast', u'taxi', u'tea', u'tell', u'temperatur', u'terrac', u'terribl', u'thank', u'thing', u'think', u'though', u'three', u'time', u'tini', u'tire', u'toilet', u'toiletri', u'top', u'touch', u'tourist', u'towel', u'tower', u'town', u'train', u'tram', u'transport', u'travel', u'tri', u'trip', u'tube', u'turn', u'tv', u'twice', u'twin', u'two', u'uncomfort', u'underground', u'understand', u'unfortun', u'upgrad', u'upon', u'use', u'valu', u'varieti', u'vienna', u'view', u'visit', u'wait', u'wake', u'walk', u'wall', u'want', u'warm', u'water', u'way', u'weekend', u'welcom', u'well', u'whole', u'wi', u'wifi', u'window', u'within', u'without', u'wonder', u'work', u'worth', u'would', u'year']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
